{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 20000000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4788bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part A:Read Novels \n",
    "def read_novels(path=Path.cwd() / \"texts\" / \"novels\"):\n",
    "    \"\"\"Reads texts from a directory of .txt files and returns a DataFrame with the text, title,\n",
    "    author, and year\"\"\"\n",
    "    texts = []\n",
    "    titles = []\n",
    "    authors = []\n",
    "    years = []\n",
    "\n",
    "    for file in path.glob(\"*.txt\"):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            texts.append(text)\n",
    "\n",
    "            filename = file.stem\n",
    "            title, author, year = filename.split('-')\n",
    "\n",
    "            titles.append(title.replace('_', ' '))\n",
    "            authors.append(author)\n",
    "            years.append(int(year))\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        \"text\": texts,\n",
    "        \"title\": titles,\n",
    "        \"author\": authors,\n",
    "        \"year\": years\n",
    "    })\n",
    "    \n",
    "    df = df.sort_values('year').reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5277d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: type-token ratio (TTR)\n",
    "def nltk_ttr(df):\n",
    "    \"\"\"Calculates the type-token ratio of a text. Returns a mapped dictionary title -> TTR.\"\"\"\n",
    "    ttr_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        tokens = word_tokenize(row['text'])\n",
    "        words = [word.lower() for word in tokens if word.isalpha()]\n",
    "        types = set(words)\n",
    "        ttr = len(types) / len(words) if len(words) > 0 else 0\n",
    "        ttr_dict[row['title']] = ttr\n",
    "\n",
    "    return ttr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72943d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part C: Flesch-Kincaid Grade Level\n",
    "def count_syl(word, d):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        word (str): The word to count syllables for.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of syllables in the word.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word in d:\n",
    "        return len([syl for syl in d[word][0] if syl[-1].isdigit()])\n",
    "    else:\n",
    "        vowels = \"aeiouy\"\n",
    "        syllable_count = 0\n",
    "        prev_char_was_vowel = False\n",
    "        \n",
    "        for char in word:\n",
    "            if char in vowels:\n",
    "                if not prev_char_was_vowel:\n",
    "                    syllable_count += 1\n",
    "                    prev_char_was_vowel = True\n",
    "            else:\n",
    "                prev_char_was_vowel = False\n",
    "        \n",
    "        if word.endswith('e') and syllable_count > 1:\n",
    "            syllable_count -= 1\n",
    "\n",
    "        return max(syllable_count, 1) \n",
    "\n",
    "def flesch_kincaid(df):\n",
    "    \"\"\"Returns the Flesch-Kincaid Grade Level of a text (higher grade is more difficult).\n",
    "    Requires a dictionary of syllables per word.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing the text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping titles to their Flesch-Kincaid Grade Level.\n",
    "    \"\"\"\n",
    "    d = cmudict.dict()\n",
    "    fk_dict = {}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = [w for w in word_tokenize(text) if w.isalpha()]\n",
    "        \n",
    "        num_sentences = len(sentences)\n",
    "        num_words = len(words)\n",
    "        num_syllables = sum(count_syl(word, d) for word in words)\n",
    "        \n",
    "        if num_words > 0 and num_sentences > 0:\n",
    "            fk_grade_level = (0.39 * (num_words / num_sentences)) + (11.8 * (num_syllables / num_words)) - 15.59\n",
    "            fk_dict[row['title']] = fk_grade_level\n",
    "        else:\n",
    "            fk_dict[row['title']] = 0\n",
    "\n",
    "    return fk_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd95bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part E: parse text with spaCy \n",
    "def parse(df, store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pkl\"):\n",
    "    \"\"\"Parses the text of a DataFrame using spaCy, stores the parsed docs as a column and writes \n",
    "    the resulting  DataFrame to a pickle file\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the text to parse.\n",
    "        store_path (Path): Path to store the pickle file.\n",
    "        out_name (str): Name of the output pickle file.\n",
    "        Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional 'parsed' column containing spaCy Doc objects.\n",
    "        \"\"\"\n",
    "    \n",
    "    df['parsed'] = df['text'].apply(nlp)\n",
    "\n",
    "    with open(store_path / out_name, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b490be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part E: Load parsed DataFrame from pickle\n",
    "def load_parsed(store_path=Path.cwd() / \"pickles\", in_name=\"parsed.pkl\"):\n",
    "    \"\"\"Loads a parsed DataFrame from a pickle file\"\"\"\n",
    "    with open(store_path / in_name, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Part F: Working with parses: \n",
    "def objects_most_common(doc):\n",
    "    \"\"\"The title of each novel and a list of the ten most common syntactic objects overall in the text.\"\"\"\n",
    "    objects = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in ('dobj', 'iobj'):\n",
    "            objects.append(token.lemma_.lower())\n",
    "    return Counter(objects).most_common(10)\n",
    "\n",
    "\n",
    "def subjects_by_verb_count(doc, verb):\n",
    "    \"\"\"\n",
    "    The title of each novel and a list of the ten most common syntactic subjects of \n",
    "    the verb ‘to hear’ (in any tense) in the text, ordered by their frequency\n",
    "    \"\"\"\n",
    "    subjects = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in ('nsubj', 'nsubjpass') and token.head.lemma_.lower() == verb.lower():\n",
    "            subjects.append(token.lemma_.lower())\n",
    "    return Counter(subjects).most_common(10)\n",
    "    \n",
    "\n",
    "def subjects_by_verb_pmi(doc, target_verb):\n",
    "    \"\"\"\n",
    "    The title of each novel and a list of the ten most common syntactic subjects of \n",
    "    the verb ‘to hear’ (in any tense) in the text, ordered by their Pointwise Mutual Information\n",
    "    \"\"\"\n",
    "    \n",
    "    subject_verb_pairs = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in ('nsubj', 'nsubjpass') and token.head.pos_ == 'VERB':\n",
    "            subject = token.lemma_.lower()\n",
    "            verb = token.head.lemma_.lower()\n",
    "            subject_verb_pairs.append((subject, verb))\n",
    "    if not subject_verb_pairs:\n",
    "        return []\n",
    "\n",
    "    # Create a BigramCollocationFinder and score the bigrams using PMI\n",
    "    finder = BigramCollocationFinder.from_documents([pair for pair in subject_verb_pairs])\n",
    "    pmi_scorer = BigramAssocMeasures.pmi\n",
    "\n",
    "    # filter for our target verb.\n",
    "    pmi_scores = finder.score_ngrams(pmi_scorer)\n",
    "    target_verb_subjects = []\n",
    "    for (subject, verb), score in pmi_scores:\n",
    "        if verb == target_verb.lower():\n",
    "            target_verb_subjects.append((subject, score))\n",
    "            \n",
    "    return target_verb_subjects[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baleid\\Desktop\\BirkBeck Study\\NLP\\Coursework\\nlp-coursework-2024-25-N-PolarStar\\p1-texts\\novels\n",
      "                                                text                  title  \\\n",
      "0  \\nCHAPTER 1\\n\\nThe family of Dashwood had long...  Sense and Sensibility   \n",
      "1  'Wooed and married and a'.'\\n'Edith!' said Mar...        North and South   \n",
      "2  Book the First--Recalled to Life\\n\\n\\n\\n\\nI. T...   A Tale of Two Cities   \n",
      "3  SAMUEL BUTLER.\\nAugust 7, 1901\\n\\nCHAPTER I: W...                Erewhon   \n",
      "4  THE AMERICAN\\n\\nby Henry James\\n\\n\\n1877\\n\\n\\n...           The American   \n",
      "\n",
      "    author  year  \n",
      "0   Austen  1811  \n",
      "1  Gaskell  1855  \n",
      "2  Dickens  1858  \n",
      "3   Butler  1872  \n",
      "4    James  1877  \n",
      "                                                text                  title  \\\n",
      "0  \\nCHAPTER 1\\n\\nThe family of Dashwood had long...  Sense and Sensibility   \n",
      "1  'Wooed and married and a'.'\\n'Edith!' said Mar...        North and South   \n",
      "2  Book the First--Recalled to Life\\n\\n\\n\\n\\nI. T...   A Tale of Two Cities   \n",
      "3  SAMUEL BUTLER.\\nAugust 7, 1901\\n\\nCHAPTER I: W...                Erewhon   \n",
      "4  THE AMERICAN\\n\\nby Henry James\\n\\n\\n1877\\n\\n\\n...           The American   \n",
      "\n",
      "    author  year                                             parsed  \n",
      "0   Austen  1811  (\\n, CHAPTER, 1, \\n\\n, The, family, of, Dashwo...  \n",
      "1  Gaskell  1855  (', Wooed, and, married, and, a, ', ., ', \\n, ...  \n",
      "2  Dickens  1858  (Book, the, First, --, Recalled, to, Life, \\n\\...  \n",
      "3   Butler  1872  (SAMUEL, BUTLER, ., \\n, August, 7, ,, 1901, \\n...  \n",
      "4    James  1877  (THE, AMERICAN, \\n\\n, by, Henry, James, \\n\\n\\n...  \n",
      "{'Sense and Sensibility': 0.052847302442989776, 'North and South': 0.0549040694681204, 'A Tale of Two Cities': 0.07072694469399422, 'Erewhon': 0.09151270564132943, 'The American': 0.06381607058523676, 'Dorian Gray': 0.08355234620193412, 'Tess of the DUrbervilles': 0.07778957979554696, 'The Golden Bowl': 0.047475476259872806, 'The Secret Garden': 0.05847231570812455, 'Portrait of the Artist': 0.10472745625841184, 'The Black Moth': 0.07866588875923765, 'Orlando': 0.1137245917497168, 'Blood Meridian': 0.08568897067593587}\n",
      "{'Sense and Sensibility': 10.873683567942479, 'North and South': 6.634860577714452, 'A Tale of Two Cities': 9.79652274605705, 'Erewhon': 14.663467774346277, 'The American': 7.906251041325181, 'Dorian Gray': 4.9366825586660745, 'Tess of the DUrbervilles': 7.606728147305784, 'The Golden Bowl': 12.42886332864489, 'The Secret Garden': 4.641180304078272, 'Portrait of the Artist': 6.424072659849031, 'The Black Moth': 4.186944830717447, 'Orlando': 9.518539259582269, 'Blood Meridian': 5.589485035678283}\n",
      "Sense and Sensibility\n",
      "[('she', 398), ('it', 380), ('he', 285), ('you', 214), ('what', 197), ('they', 192), ('i', 175), ('which', 114), ('herself', 94), ('nothing', 79)]\n",
      "\n",
      "\n",
      "North and South\n",
      "[('it', 556), ('he', 483), ('she', 455), ('i', 354), ('what', 352), ('you', 277), ('they', 187), ('which', 170), ('herself', 146), ('margaret', 120)]\n",
      "\n",
      "\n",
      "A Tale of Two Cities\n",
      "[('it', 451), ('he', 411), ('you', 240), ('i', 233), ('she', 197), ('they', 167), ('what', 153), ('himself', 132), ('hand', 125), ('that', 111)]\n",
      "\n",
      "\n",
      "Erewhon\n",
      "[('it', 230), ('i', 222), ('they', 158), ('which', 151), ('he', 104), ('what', 101), ('that', 81), ('she', 48), ('myself', 47), ('we', 41)]\n",
      "\n",
      "\n",
      "The American\n",
      "[('it', 583), ('you', 479), ('i', 395), ('he', 370), ('she', 304), ('what', 292), ('that', 182), ('they', 149), ('hand', 144), ('something', 114)]\n",
      "\n",
      "\n",
      "Dorian Gray\n",
      "[('it', 300), ('i', 256), ('he', 252), ('you', 188), ('that', 178), ('what', 166), ('they', 98), ('she', 92), ('thing', 68), ('something', 52)]\n",
      "\n",
      "\n",
      "Tess of the DUrbervilles\n",
      "[('she', 497), ('it', 425), ('he', 298), ('i', 248), ('you', 239), ('what', 185), ('they', 171), ('that', 128), ('which', 121), ('herself', 90)]\n",
      "\n",
      "\n",
      "The Golden Bowl\n",
      "[('it', 1174), ('she', 705), ('what', 654), ('he', 537), ('they', 308), ('i', 255), ('you', 237), ('that', 224), ('herself', 183), ('nothing', 125)]\n",
      "\n",
      "\n",
      "The Secret Garden\n",
      "[('it', 368), ('he', 220), ('she', 168), ('what', 135), ('i', 111), ('thing', 95), ('they', 94), ('you', 89), ('that', 61), ('hand', 59)]\n",
      "\n",
      "\n",
      "Portrait of the Artist\n",
      "[('he', 295), ('it', 180), ('they', 103), ('what', 102), ('you', 91), ('i', 75), ('hand', 66), ('that', 57), ('which', 56), ('eye', 50)]\n",
      "\n",
      "\n",
      "The Black Moth\n",
      "[('he', 405), ('you', 359), ('it', 344), ('i', 341), ('she', 269), ('hand', 126), ('what', 116), ('that', 102), ('_', 98), ('himself', 83)]\n",
      "\n",
      "\n",
      "Orlando\n",
      "[('it', 192), ('she', 162), ('he', 149), ('they', 94), ('which', 85), ('what', 70), ('that', 46), ('himself', 41), ('herself', 41), ('we', 39)]\n",
      "\n",
      "\n",
      "Blood Meridian\n",
      "[('it', 415), ('he', 291), ('they', 290), ('horse', 164), ('what', 110), ('head', 105), ('man', 91), ('hand', 86), ('pistol', 72), ('one', 54)]\n",
      "\n",
      "\n",
      "Sense and Sensibility\n",
      "[('i', 32), ('you', 19), ('she', 14), ('they', 6), ('elinor', 6), ('he', 6), ('jennings', 3), ('we', 2), ('brandon', 1), ('both', 1)]\n",
      "\n",
      "\n",
      "North and South\n",
      "[('she', 60), ('i', 47), ('he', 23), ('you', 15), ('they', 14), ('margaret', 10), ('we', 5), ('thornton', 3), ('who', 3), ('yo', 2)]\n",
      "\n",
      "\n",
      "A Tale of Two Cities\n",
      "[('i', 23), ('he', 20), ('you', 12), ('she', 11), ('they', 5), ('monseigneur', 2), ('one', 1), ('jerry', 1), ('stranger', 1), ('clink', 1)]\n",
      "\n",
      "\n",
      "Erewhon\n",
      "[('i', 38), ('he', 4), ('they', 3), ('she', 2), ('we', 1), ('who', 1), ('destruction', 1), ('machine', 1), ('one', 1)]\n",
      "\n",
      "\n",
      "The American\n",
      "[('he', 18), ('i', 14), ('you', 10), ('she', 5), ('newman', 4), ('they', 2), ('we', 2), ('who', 1), ('one', 1), ('all', 1)]\n",
      "\n",
      "\n",
      "Dorian Gray\n",
      "[('i', 24), ('he', 16), ('one', 3), ('you', 3), ('lover', 1), ('hast', 1), ('who', 1), ('jar', 1), ('dorian', 1)]\n",
      "\n",
      "\n",
      "Tess of the DUrbervilles\n",
      "[('she', 37), ('i', 20), ('they', 12), ('he', 8), ('you', 8), ('who', 6), ('tess', 4), ('clare', 4), ('lady', 2), ('footstep', 1)]\n",
      "\n",
      "\n",
      "The Golden Bowl\n",
      "[('she', 16), ('he', 8), ('you', 5), ('maggie', 2), ('man', 1), ('they', 1), ('it', 1), ('who', 1), ('amerigo', 1), ('which', 1)]\n",
      "\n",
      "\n",
      "The Secret Garden\n",
      "[('i', 28), ('she', 25), ('he', 16), ('you', 8), ('we', 6), ('mary', 3), ('they', 3), ('lennox', 2), ('colin', 2), ('one', 2)]\n",
      "\n",
      "\n",
      "Portrait of the Artist\n",
      "[('he', 62), ('you', 12), ('i', 9), ('stephen', 5), ('voice', 3), ('they', 2), ('who', 2), ('boy', 1), ('rattle', 1), ('burst', 1)]\n",
      "\n",
      "\n",
      "The Black Moth\n",
      "[('i', 37), ('he', 5), ('you', 5), ('we', 4), ('richard', 2), ('she', 2), ('nothing', 1), ('street', 1), ('it', 1), (\"ye've\", 1)]\n",
      "\n",
      "\n",
      "Orlando\n",
      "[('she', 22), ('he', 8), ('orlando', 5), ('they', 3), ('one', 3), ('i', 2), ('hoof', 1), ('it', 1), ('footfall', 1), ('noise', 1)]\n",
      "\n",
      "\n",
      "Blood Meridian\n",
      "[('he', 25), ('they', 21), ('i', 6), ('you', 5), ('who', 3), ('she', 2), ('all', 2), ('we', 2), ('man', 2), ('nobody', 1)]\n",
      "\n",
      "\n",
      "Sense and Sensibility\n",
      "[('dissent', 6.526786886439062), ('footstep', 6.526786886439062), ('both', 4.526786886439062), ('much', 3.7194319643814575), ('herself', 2.82634716829797), ('palmer', 1.7718993842755932), ('jennings', 1.49703954304501), ('you', 1.4460394725547001), ('nothing', 1.3568618849967495), ('one', 1.2788593729954763)]\n",
      "\n",
      "\n",
      "North and South\n",
      "[('tramp', 6.369057516509651), ('clank', 6.369057516509651), ('itself', 6.369057516509651), ('step', 4.784095015788495), ('nobody', 4.369057516509651), ('fool', 4.047129421622288), ('world', 3.7840950157884947), ('neighbour', 3.561702594452047), ('papa', 1.9096258978723535), ('lady', 1.561702594452047)]\n",
      "\n",
      "\n",
      "A Tale of Two Cities\n",
      "[('stranger', 6.888674850413435), ('clink', 6.888674850413435), ('jar', 6.888674850413435), ('d’ye', 6.888674850413435), ('matter', 5.888674850413435), ('echo', 4.566746755526072), ('mother', 3.8886748504134347), ('sound', 3.8886748504134347), ('voice', 3.7187498489711226), ('his', 3.4292432317761374)]\n",
      "\n",
      "\n",
      "Erewhon\n",
      "[('destruction', 5.904113510937945), ('machine', 2.4446818923006473), ('i', 1.5172299742098123), ('she', 0.8928862555146905), ('one', 0.274756890858335), ('he', -0.15116892456324493), ('who', -0.7397426788367802), ('they', -0.9434658887291578), ('we', -1.4795907815361078)]\n",
      "\n",
      "\n",
      "The American\n",
      "[('all', 5.580063701308317), ('duchess', 3.9019917961956794), ('one', 1.857597676837226), ('we', 1.01324854729742), ('he', 0.8436993553445569), ('you', 0.6018680716266653), ('they', 0.39023914242829977), ('who', 0.29466148244606877), ('i', 0.24786727096712757), ('she', -0.11481649149087453)]\n",
      "\n",
      "\n",
      "Dorian Gray\n",
      "[('hast', 7.202556005682279), ('lover', 6.202556005682279), ('jar', 6.202556005682279), ('one', 1.7107029093526043), ('i', 1.2757658526360556), ('he', 0.8145387203371445), ('dorian', 0.5301306637107837), ('who', -0.22370874901981888), ('you', -0.9146541789621124)]\n",
      "\n",
      "\n",
      "Tess of the DUrbervilles\n",
      "[('step', 6.702057218956995), ('overhead', 6.702057218956995), ('rush', 5.702057218956995), ('mansion', 5.702057218956995), ('queen', 5.117094718235839), ('latter', 5.117094718235839), ('lady', 4.702057218956995), ('footstep', 4.117094718235839), ('room', 4.117094718235839), ('brooks', 3.3801291240696325)]\n",
      "\n",
      "\n",
      "The Golden Bowl\n",
      "[('amerigo', 3.426826504968476), ('man', 3.3626961675487608), ('which', 1.5643300287184112), ('maggie', 1.3984180897827223), ('who', 1.2715482794905655), ('she', 0.9978953824351295), ('you', 0.7903337036072382), ('he', 0.6206430771213164), ('they', -0.8159145617597741), ('it', -1.4800640906400422)]\n",
      "\n",
      "\n",
      "The Secret Garden\n",
      "[(\"one'll\", 6.37060815744353), ('whoever', 5.37060815744353), ('us', 4.37060815744353), ('lennox', 4.200683156001218), ('chap', 4.048680062556167), ('roach', 3.563253235385926), (\"tha'd\", 3.2006831560012174), ('sound', 3.048680062556168), ('father', 2.2006831560012174), ('we', 2.1226806439999444)]\n",
      "\n",
      "\n",
      "Portrait of the Artist\n",
      "[('rattle', 5.951443680868834), ('listener', 5.951443680868834), ('his', 5.951443680868834), ('frail', 5.951443680868834), ('burst', 4.951443680868834), ('newman', 4.951443680868834), ('whistle', 4.3664811801476775), ('boot', 4.3664811801476775), ('confession', 3.9514436808688336), ('anyone', 3.9514436808688336)]\n",
      "\n",
      "\n",
      "The Black Moth\n",
      "[('street', 7.206977885264322), ('bel', 7.206977885264322), ('step', 6.206977885264322), (\"ye've\", 5.206977885264322), ('madam', 4.6220153845431655), ('bettison', 3.885049790376959), ('nothing', 3.7475462666270243), ('voice', 3.300087289655803), ('we', 2.479057430701122), ('_', 1.8146604624855611)]\n",
      "\n",
      "\n",
      "Orlando\n",
      "[('hoof', 6.713297623387024), ('footfall', 6.713297623387024), ('noise', 6.713297623387024), ('cry', 6.713297623387024), (\"man,'--orlando\", 6.713297623387024), ('god', 4.713297623387024), ('child', 4.391369528499661), ('boy', 3.5433726219447115), ('reader', 2.90594270132942), ('some', 2.6258347821366845)]\n",
      "\n",
      "\n",
      "Blood Meridian\n",
      "[('nobody', 5.2511780481440065), ('all', 3.2067839287855535), ('who', 1.8027175473277124), ('rider', 1.607321858369282), ('she', 1.3931970530164346), ('they', 1.0463740421331984), ('i', 0.993790205451355), ('you', 0.8657470109504863), ('he', 0.586695207779324), ('we', 0.5646775209607882)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = Path.cwd() / \"p1-texts\" / \"novels\"\n",
    "print(path)\n",
    "df = read_novels(path) \n",
    "print(df.head())\n",
    "\n",
    "parse(df)\n",
    "print(df.head())\n",
    "print(nltk_ttr(df))\n",
    "print(flesch_kincaid(df))\n",
    "\n",
    "df = pd.read_pickle(Path.cwd() / \"pickles\" /\"parsed.pkl\")\n",
    "\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(row[\"title\"])\n",
    "    print(objects_most_common(row[\"parsed\"]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(row[\"title\"])\n",
    "    print(subjects_by_verb_count(row[\"parsed\"], \"hear\"))\n",
    "    print(\"\\n\")\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(row[\"title\"])\n",
    "    print(subjects_by_verb_pmi(row[\"parsed\"], \"hear\"))\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

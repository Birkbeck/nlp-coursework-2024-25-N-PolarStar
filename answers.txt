Answers to the text questions go here.
Part 1 
    Question (d):
    Answer:
        1. The Flesch-Kincaid score doesn’t always reflect real reading difficulty, especially for poetry or literary novels. 
        These types of texts often use unusual sentence structures, creative punctuation, or figurative language that the 
        formula isn’t designed to handle. This can lead to misleading scores, making the text seem much easier or harder 
        than it actually is if the writing style doesn’t follow standard rules.
        2. Another limitation is that the Flesch-Kincaid score doesn’t consider whether the vocabulary is common or rare. 
        Even if a text is full of technical or specialized words that most people don’t know, it might still get a 
        low (easy) score if the words themselves are short. The formula doesn’t check if the vocabulary is familiar 
        or used in everyday language, so it can underestimate the difficulty of texts in fields like science or law.

Part 2
    Question (F):

    The custom tokenizer leverages the spaCy English model to perform text processing. For each document, it first 
    filters tokens to retain only alphabetic words that are not stop words. It then performs Part-of-Speech (POS) filtering, 
    isolating only nouns and verbs and exceed two characters in length. The resulting tokens are lemmatized to their dictionary base
    form. This entire process is used within a TfidfVectorizer configured with min_df=2, which ensures that the final vocabulary 
    only contains tokens that appear in at least two separate documents.

    The custom tokenizer resulted in slower performance and slightly worse results compared to the default tokenizer. With Random Forest, 
    the macro F1 dropped from 0.45 (default) to 0.37 (custom), and SVM macro F1 dropped from 0.58 to 0.50. The drop in performance was most 
    pronounced for minority classes like Liberal Democrat and Scottish National Party, which showed very low recall and F1 scores with the 
    custom tokenizer. Additionally, the custom approach was much slower, taking about 12 minutes to process compared to just 1.5 minutes for 
    the default tokenizer, due to the computational overhead of running spaCy on every document. Therefore, the default tokenizer outperformed
    the custom tokenizer. 